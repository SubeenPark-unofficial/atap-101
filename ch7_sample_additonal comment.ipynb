{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "import argparse\n",
    "from functools import partial\n",
    "\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk import ngrams as nltk_ngrams\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'The latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations.'\n",
    "words = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams(words, n=2): \n",
    "    for idx in range(len(words)-n+1):  #[w0, w1, w2, w3][2:4] -> [w0, w1], [w1, w2], [w2, w3]\n",
    "        yield tuple(words[idx:idx+n])\n",
    "\n",
    "LPAD_SYMBOL = \"<s>\"\n",
    "RPAD_SYMBOL = \"</s>\"\n",
    "\n",
    "nltk_ngrams = partial(nltk_ngrams,\n",
    "    pad_right=True, pad_left=True,\n",
    "    right_pad_symbol=RPAD_SYMBOL, left_pad_symbol=LPAD_SYMBOL\n",
    "    )\n",
    "\n",
    "def ngrams2(text, n=2):\n",
    "    for sent in sent_tokenize(text):\n",
    "        sent = word_tokenize(sent)\n",
    "        for ngram in nltk_ngrams(sent, n):\n",
    "            yield ngram\n",
    "            \n",
    "def ngrams3(words, n=2):\n",
    "        for ngram in nltk_ngrams(words, n):\n",
    "            yield ngram            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ngram in ngrams(words, n=2):\n",
    "    print(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ngram in ngrams2(words, n=2):\n",
    "    print(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ngram in ngrams2(text, n=3):\n",
    "    print(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ngram in ngrams3(words, n=3):\n",
    "    print(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58,748 vocabulary 1,624,862 word count\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "from nltk.corpus.reader.api import CorpusReader\n",
    "from nltk.corpus.reader.api import CategorizedCorpusReader\n",
    "\n",
    "DOC_PATTERN = r'(?!\\.)[a-z_\\s]+/[a-f0-9]+\\.json'\n",
    "PKL_PATTERN = r'(?!\\.)[a-z_\\s]+/[a-f0-9]+\\.pickle'\n",
    "CAT_PATTERN = r'([a-z_\\s]+)/.*'\n",
    "\n",
    "\n",
    "class PickledCorpusReader(CategorizedCorpusReader, CorpusReader):\n",
    "\n",
    "    def __init__(self, root, fileids=PKL_PATTERN, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the corpus reader.  Categorization arguments\n",
    "        (``cat_pattern``, ``cat_map``, and ``cat_file``) are passed to\n",
    "        the ``CategorizedCorpusReader`` constructor.  The remaining arguments\n",
    "        are passed to the ``CorpusReader`` constructor.\n",
    "        \"\"\"\n",
    "        # Add the default category pattern if not passed into the class.\n",
    "        if not any(key.startswith('cat_') for key in kwargs.keys()):\n",
    "            kwargs['cat_pattern'] = CAT_PATTERN\n",
    "\n",
    "        CategorizedCorpusReader.__init__(self, kwargs)\n",
    "        CorpusReader.__init__(self, root, fileids)\n",
    "\n",
    "        self._word_tokenizer = WordPunctTokenizer()\n",
    "        self._sent_tokenizer = nltk.data.LazyLoader(\n",
    "            'tokenizers/punkt/english.pickle')\n",
    "\n",
    "    def _resolve(self, fileids, categories):\n",
    "        \"\"\"\n",
    "        Returns a list of fileids or categories depending on what is passed\n",
    "        to each internal corpus reader function. This primarily bubbles up to\n",
    "        the high level ``docs`` method, but is implemented here similar to\n",
    "        the nltk ``CategorizedPlaintextCorpusReader``.\n",
    "        \"\"\"\n",
    "        if fileids is not None and categories is not None:\n",
    "            raise ValueError(\"Specify fileids or categories, not both\")\n",
    "\n",
    "        if categories is not None:\n",
    "            return self.fileids(categories)\n",
    "        return fileids\n",
    "\n",
    "    def feeds(self):\n",
    "        data = self.open('feeds.json')\n",
    "        return json.load(data)\n",
    "\n",
    "    def docs(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns the document loaded from a pickled object for every file in\n",
    "        the corpus. Similar to the BaleenCorpusReader, this uses a generator\n",
    "        to acheive memory safe iteration.\n",
    "        \"\"\"\n",
    "        # Resolve the fileids and the categories\n",
    "        fileids = self._resolve(fileids, categories)\n",
    "\n",
    "        # Create a generator, loading one document into memory at a time.\n",
    "        for path, enc, fileid in self.abspaths(fileids, True, True):\n",
    "            with open(path, 'rb') as f:\n",
    "                yield pickle.load(f)\n",
    "\n",
    "    def paras(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns a generator of paragraphs where each paragraph is a list of\n",
    "        sentences, which is in turn a list of (token, tag) tuples.\n",
    "        \"\"\"\n",
    "        for doc in self.docs(fileids, categories):\n",
    "            for paragraph in doc:\n",
    "                yield paragraph\n",
    "\n",
    "    def sents(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns a generator of sentences where each sentence is a list of\n",
    "        (token, tag) tuples.\n",
    "        \"\"\"\n",
    "        for paragraph in self.paras(fileids, categories):\n",
    "            for sentence in paragraph:\n",
    "                yield sentence\n",
    "\n",
    "    def tagged(self, fileids=None, categories=None):\n",
    "        for sent in self.sents(fileids, categories):\n",
    "            for token in sent:\n",
    "                yield token\n",
    "\n",
    "    def words(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns a generator of (token, tag) tuples.\n",
    "        \"\"\"\n",
    "        for sentence in self.sents(fileids, categories):\n",
    "            for token, _ in sentence:\n",
    "                yield token\n",
    "\n",
    "    def describe(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Performs a single pass of the corpus and\n",
    "        returns a dictionary with a variety of metrics\n",
    "        concerning the state of the corpus.\n",
    "        \"\"\"\n",
    "        # Structures to perform counting.\n",
    "        counts  = nltk.FreqDist()\n",
    "        tokens  = nltk.FreqDist()\n",
    "\n",
    "        # Perform single pass over paragraphs, tokenize and count\n",
    "        for para in self.paras(fileids, categories):\n",
    "            for sent in para:\n",
    "                for word, tag in sent:\n",
    "                    counts['words'] += 1 # total word count\n",
    "                    tokens[word] += 1 # incidents of each word\n",
    "\n",
    "        # Return data structure with information\n",
    "        return {\n",
    "            'words':  counts['words'], # word count\n",
    "            'vocab':  len(tokens), # unique word count / token count\n",
    "            'lexdiv': float(counts['words']) / float(len(tokens)), \n",
    "        }\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    from collections import Counter\n",
    "\n",
    "    corpus = PickledCorpusReader('corpus')\n",
    "    words = Counter(corpus.words())\n",
    "\n",
    "    print(\"{:,} vocabulary {:,} word count\".format(\n",
    "        len(words.keys()), sum(words.values())\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar with 13 productions (start state = S)\n",
      "    S -> NNP VP\n",
      "    VP -> V PP\n",
      "    PP -> P NP\n",
      "    NP -> DT N\n",
      "    NNP -> 'Gwen'\n",
      "    NNP -> 'George'\n",
      "    V -> 'looks'\n",
      "    V -> 'burns'\n",
      "    P -> 'in'\n",
      "    P -> 'for'\n",
      "    DT -> 'the'\n",
      "    N -> 'castle'\n",
      "    N -> 'ocean'\n",
      "S\n",
      "[S -> NNP VP, VP -> V PP, PP -> P NP, NP -> DT N, NNP -> 'Gwen', NNP -> 'George', V -> 'looks', V -> 'burns', P -> 'in', P -> 'for', DT -> 'the', N -> 'castle', N -> 'ocean']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "GRAMMAR = \"\"\"\n",
    "    S -> NNP VP\n",
    "    VP -> V PP\n",
    "    PP -> P NP\n",
    "    NP -> DT N\n",
    "    NNP -> 'Gwen' | 'George'\n",
    "    V -> 'looks' | 'burns'\n",
    "    P -> 'in' | 'for'\n",
    "    DT -> 'the'\n",
    "    N -> 'castle' | 'ocean'\n",
    "    \"\"\"\n",
    "\n",
    "cfg = nltk.CFG.fromstring(GRAMMAR)\n",
    "\n",
    "print(cfg)\n",
    "print(cfg.start())\n",
    "print(cfg.productions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# collocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from collections import defaultdict\n",
    "\n",
    "from nltk.collocations import QuadgramCollocationFinder\n",
    "from nltk.metrics.association import QuadgramAssocMeasures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_quadgrams(corpus, metric, path=None):\n",
    "    \"\"\"\n",
    "    Find and rank quadgrams from the supplied corpus using the given\n",
    "    association metric. Write the quadgrams out to the given path if\n",
    "    supplied otherwise return the list in memory.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a collocation ranking utility from corpus words.\n",
    "    ngrams = QuadgramCollocationFinder.from_words(corpus.words())\n",
    "\n",
    "    # Rank collocations by an association metric\n",
    "    scored = ngrams.score_ngrams(metric)\n",
    "\n",
    "    if path:\n",
    "        with open(path, 'w') as f:\n",
    "            f.write(\"Collocation\\tScore ({})\\n\".format(metric.__name__))\n",
    "            for ngram, score in scored:\n",
    "                f.write(\"{}\\t{}\\n\".format(repr(ngram), score))\n",
    "    else:\n",
    "        return scored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "corpus = PickledCorpusReader('corpus_small')\n",
    "rank_quadrams_small = rank_quadgrams(corpus, QuadgramAssocMeasures)\n",
    "\n",
    "\n",
    "\n",
    "    # # Group quadgrams by first word\n",
    "    # prefixed = defaultdict(list)\n",
    "    # for key, score in scored:\n",
    "    #     prefixed[key[0]].append((key[1:], scores))\n",
    "    #\n",
    "    # # Sort keyed quadgrams by strongest association\n",
    "    # for key in prefixed:\n",
    "    #     prefixed[key].sort(key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_quadrams_small[1: 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_quadrams_small[-10:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ne_chunk\n",
    "from itertools import groupby\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.chunk import tree2conlltags\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.chunk.regexp import RegexpParser\n",
    "from unicodedata import category as unicat\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAMMAR = r'KT: {(<JJ>* <NN.*>+ <IN>)? <JJ>* <NN.*>+}'\n",
    "GOODTAGS = frozenset(['JJ','JJR','JJS','NN','NNP','NNS','NNPS'])\n",
    "GOODLABELS = frozenset(['PERSON', 'ORGANIZATION', 'FACILITY', 'GPE', 'GSP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeyphraseExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Wraps a PickledCorpusReader consisting of pos-tagged documents.\n",
    "    \"\"\"\n",
    "    def __init__(self, grammar=GRAMMAR):\n",
    "        self.grammar = GRAMMAR\n",
    "        self.chunker = RegexpParser(self.grammar)\n",
    "\n",
    "    def normalize(self, sent):\n",
    "        \"\"\"\n",
    "        Removes punctuation from a tokenized/tagged sentence and\n",
    "        lowercases words.\n",
    "        \"\"\"\n",
    "        is_punct = lambda word: all(unicat(char).startswith('P') for char in word)\n",
    "        sent = filter(lambda t: not is_punct(t[0]), sent)\n",
    "        sent = map(lambda t: (t[0].lower(), t[1]), sent)\n",
    "        return list(sent)\n",
    "\n",
    "    def extract_keyphrases(self, document):\n",
    "        \"\"\"\n",
    "        For a document, parse sentences using our chunker created by\n",
    "        our grammar, converting the parse tree into a tagged sequence.\n",
    "        Yields extracted phrases.\n",
    "        \"\"\"\n",
    "        for sents in document:\n",
    "            for sent in sents:\n",
    "                sent = self.normalize(sent)\n",
    "                if not sent: continue\n",
    "                chunks = tree2conlltags(self.chunker.parse(sent))\n",
    "                phrases = [ \" \".join(word for word, pos, chunk in group).lower() \n",
    "                # {true: group1, false:group2}\n",
    "                for key, group in groupby(chunks, lambda term: term[-1] != 'O')  # groupby(input, key)\n",
    "                if key\n",
    "                ]                \n",
    "                for phrase in phrases:\n",
    "                    yield phrase\n",
    "\n",
    "    def fit(self, documents, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, documents):\n",
    "        for document in documents:\n",
    "            yield list(self.extract_keyphrases(document))\n",
    "\n",
    "\n",
    "class EntityExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, labels=GOODLABELS, **kwargs):\n",
    "        self.labels = labels\n",
    "\n",
    "    def get_entities(self, document):\n",
    "        entities = []\n",
    "        for paragraph in document:\n",
    "            for sentence in paragraph:\n",
    "                trees = ne_chunk(sentence)\n",
    "                for tree in trees:\n",
    "                    if hasattr(tree, 'label'):\n",
    "                        if tree.label() in self.labels:\n",
    "                            entities.append(\n",
    "                                ' '.join([child[0].lower() for child in tree]) # child[0] : 단어\n",
    "                                )\n",
    "        return entities\n",
    "\n",
    "    def fit(self, documents, labels=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, documents):\n",
    "        for document in documents:\n",
    "            yield self.get_entities(document)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "corpus = PickledCorpusReader('corpus')\n",
    "docs = corpus.docs()\n",
    "\n",
    "# phrase_extractor = KeyphraseExtractor()\n",
    "# keyphrases = list(phrase_extractor.fit_transform(docs))\n",
    "# print(keyphrases[0])\n",
    "\n",
    "# entity_extractor = EntityExtractor()\n",
    "# entities = list(entity_extractor.fit_transform(docs))\n",
    "# print(entities[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from math import log\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "from nltk.util import ngrams\n",
    "from nltk.probability import ProbDistI, FreqDist, ConditionalFreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NgramCounter(object): #NgramCounter(2, vocab)\n",
    "    \"\"\"\n",
    "    The NgramCounter class counts ngrams given a vocabulary and ngram size.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n, vocabulary, unknown=\"<UNK>\"):\n",
    "        \"\"\"\n",
    "        n is the size of the ngram\n",
    "        변수들을 초기화하는 역할. Constructor에 input(vocabulary, unknown)을 넣어주면 그 값에 맞춰서 변수의 값이 설정된다. \n",
    "        \"\"\"\n",
    "        if n < 1:\n",
    "            raise ValueError(\"ngram size must be greater than or equal to 1\")\n",
    "\n",
    "        self.n = n\n",
    "        self.unknown = unknown\n",
    "        self.padding = {\n",
    "            \"pad_left\": True,\n",
    "            \"pad_right\": True,\n",
    "            \"left_pad_symbol\": \"<s>\",\n",
    "            \"right_pad_symbol\": \"</s>\"\n",
    "        }\n",
    "\n",
    "        self.vocabulary = vocabulary\n",
    "        self.allgrams = defaultdict(ConditionalFreqDist) # dictionary의 기본 value 형태가 Conditional FreqDist 형태로 설정\n",
    "        self.ngrams = FreqDist() \n",
    "        self.unigrams = FreqDist() \n",
    "\n",
    "        \n",
    "    \n",
    "    def train_counts(self, training_text):\n",
    "        \n",
    "        [['Hello', 'Word'], [\"Life\", 'is', 'short']]\n",
    "        \n",
    "        for sent in training_text:\n",
    "            \n",
    "            #sent 내부의 word가 self.vocabulary에 존재하는 단어인지 확인 후 있으면 단어를, 아니면 self.unknown 을 저장\n",
    "\n",
    "        \n",
    "            checked_sent = (self.check_against_vocab(word) for word in sent)  #vocab check\n",
    "            \n",
    "            \n",
    "            sent_start = True # unigram counting에서 한 단어가 여러번 세지는거 방지\n",
    "            \n",
    "            # [w1, w2, w3]\n",
    "            for ngram in self.to_ngrams(checked_sent): # checked_sent 로부터 ngram list을 생성\n",
    "                \n",
    "\n",
    "                self.ngrams[ngram] += 1 # 각 ngram의 개수를 count\n",
    "                # (w1, w2) , w3\n",
    "                context, word = tuple(ngram[:-1]), ngram[-1] # ngram을 분리: 맨 마지막 단어가 word 가 되고 그 앞의 단어들은 모두 context\n",
    "                \n",
    "                \n",
    "                # unigram counting\n",
    "                if sent_start: # for loop가 돌 때마다 unigram counting이 일어나는 것을 방지\n",
    "                    for context_word in context:\n",
    "                        self.unigrams[context_word] += 1 \n",
    "                    sent_start = False\n",
    "                    \n",
    "                # conditional frequency\n",
    "                # w1, w2 \n",
    "                for window, ngram_order in enumerate(range(self.n, 1, -1)): # [3, 2] -> (0, 3), (1, 2)\n",
    "                    context = context[window:] \n",
    "                    self.allgrams[ngram_order][context][word] += 1  #conditional frequency counting\n",
    "                self.unigrams[word] += 1 \n",
    "                \n",
    "                \n",
    "\n",
    "    def check_against_vocab(self, word):\n",
    "        if word in self.vocabulary:\n",
    "            return word\n",
    "        return self.unknown\n",
    "\n",
    "    def to_ngrams(self, sequence):\n",
    "        \"\"\"\n",
    "        Wrapper for NLTK ngrams method\n",
    "        \"\"\"\n",
    "        return ngrams(sequence, self.n, **self.padding) # 앞에 **이 붙는 것은 keyword argument 형태로 변수를 넣어주겠다는 것! 주로 여러개의 변수를 한번에 넣을 때 사용하는 방식\n",
    "\n",
    "\n",
    "def count_ngrams(n, vocabulary, texts):\n",
    "    counter = NgramCounter(n, vocabulary) # Constructor를 이용해 instance 생성, 이 때 __init__ 함수가 실행된다. -> unigram/ngrams/allgrams 설정 \n",
    "    counter.train_counts(texts) # texts를 이용해서 unigram/ngrams/allgrams count\n",
    "    return counter # UNIGRAM/NGRAM/ALLGRAMS(conditional)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "corpus = PickledCorpusReader('corpus')\n",
    "# tokens = [''.join(word[0]) for word in corpus.words()]\n",
    "tokens = list(corpus.words())\n",
    "vocab = Counter(tokens)\n",
    "sents = list([word[0] for word in sent] for sent in corpus.sents())\n",
    "trigram_counts = count_ngrams(3, vocab, sents) # UNIGRAM/NGRAM/ALLGRAMS(conditional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 58750 samples and 1924458 outcomes>\n",
      "<FreqDist with 1151450 samples and 1774660 outcomes>\n",
      "[\"'\", 'Source', 'and', 'nominates', 'in', 'as', 'said', 'is', 'who', 'that', '.', 'are', 'called', 'announced', 'isn', '’', 'for', 'would', 'directly']\n"
     ]
    }
   ],
   "source": [
    "print(trigram_counts.unigrams) \n",
    "print(trigram_counts.ngrams)\n",
    "# print(trigram_counts.allgrams[3]) \n",
    "# print(sorted(trigram_counts.allgrams[3].conditions())) CondFreqDist\n",
    "print(list(trigram_counts.allgrams[3][('the', 'President')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = PickledCorpusReader('corpus_small')\n",
    "\n",
    "class BaseNgramModel(object):\n",
    "    \"\"\"\n",
    "    The BaseNgramModel creates an n-gram language model.\n",
    "    This base model is equivalent to a Maximum Likelihood Estimation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ngram_counter):\n",
    "        \"\"\"\n",
    "        BaseNgramModel is initialized with an NgramCounter.\n",
    "        \"\"\"\n",
    "        self.n = ngram_counter.n \n",
    "        self.ngram_counter = ngram_counter\n",
    "        self.ngrams = ngram_counter.ngrams\n",
    "        self._check_against_vocab = self.ngram_counter.check_against_vocab\n",
    "\n",
    "    def check_context(self, context):\n",
    "        \"\"\"\n",
    "        Ensures that the context is not longer than or equal to the model's\n",
    "        n-gram order.\n",
    "        Returns the context as a tuple.\n",
    "        \"\"\"\n",
    "        if len(context) >= self.n:\n",
    "            raise ValueError(\"Context too long for this n-gram\")\n",
    "\n",
    "        return tuple(context)\n",
    "\n",
    "    def score(self, word, context):\n",
    "        \"\"\"\n",
    "        For a given string representation of a word, and a string word context,\n",
    "        returns the maximum likelihood score that the word will follow the\n",
    "        context.\n",
    "        \"\"\"\n",
    "        context = self.check_context(context)\n",
    "\n",
    "        return self.ngrams[context].freq(word) # (context 이후에 word 가 나오는 frequency) / (전체 data에서 word의 frequency)\n",
    "\n",
    "    def logscore(self, word, context):\n",
    "        \"\"\"\n",
    "        For a given string representation of a word, and a word context,\n",
    "        computes the log probability of this word in this context.\n",
    "        \n",
    "        \"\"\"\n",
    "        score = self.score(word, context)\n",
    "        if score == 0.0:\n",
    "            return float(\"-inf\")\n",
    "\n",
    "        return log(score, 2)\n",
    "\n",
    "    def entropy(self, text):\n",
    "        \"\"\"\n",
    "        Calculate the approximate cross-entropy of the n-gram model for a\n",
    "        given text represented as a list of comma-separated strings.\n",
    "        This is the average log probability of each word in the text.\n",
    "        \"\"\"\n",
    "        # entropy 참고: https://velog.io/@hojp7874/교차-엔트로피\n",
    "        \n",
    "        normed_text = (self._check_against_vocab(word) for word in text) # vocab check\n",
    "        entropy = 0.0\n",
    "        processed_ngrams = 0\n",
    "        for ngram in self.ngram_counter.to_ngrams(normed_text): # normed text로부터 ngram 목록을 생성\n",
    "            context, word = tuple(ngram[:-1]), ngram[-1] # 각 ngram을 context와 ngram으로 분리\n",
    "            entropy += self.logscore(word, context) \n",
    "            processed_ngrams += 1 \n",
    "            \n",
    "        # entropy: logscore의 합 / processed_ngrams: 식에서 n\n",
    "        return - (entropy / processed_ngrams) # 모든 Ngram의 word에 대해 가능한 평균 entropy\n",
    "\n",
    "    def perplexity(self, text):\n",
    "        \"\"\"\n",
    "        Given list of comma-separated strings, calculates the perplexity\n",
    "        of the text.\n",
    "        \"\"\"\n",
    "        return pow(2.0, self.entropy(text))\n",
    "\n",
    "    \n",
    "# context + word가 기존 corpus에 존재하지 않을 수 있다! 그 경우를 해결하기 위한 것이 Add-K method\n",
    "# Add-K method의 경우 기존에 train data set에 없는 단어가 등장 시 이전에 K 번 등장했다고 생각하고 frequency를 K로 두게 된다\n",
    "# 이 중에서도 K=1 인 경우가 바로 add-one, Laplace smoothing\n",
    "\n",
    "class AddKNgramModel(BaseNgramModel):\n",
    "    \"\"\"\n",
    "    Provides Add-k-smoothed scores.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, k, *args):\n",
    "        \"\"\"\n",
    "        Expects an input value, k, a number by which\n",
    "        to increment word counts during scoring.\n",
    "        \"\"\"\n",
    "        super(AddKNgramModel, self).__init__(*args)\n",
    "\n",
    "        self.k = k\n",
    "        self.k_norm = len(self.ngram_counter.vocabulary) * k\n",
    "        \n",
    "    \n",
    "    # 기존에 score를 계산하던 방식을 add - K 방식에 맞춰 덮어씌운다\n",
    "    # 기존 방식: (context 이후에 word 가 나오는 frequency) / (전체 data에서 word의 frequenc\n",
    "    def score(self, word, context):\n",
    "        \"\"\"\n",
    "        With Add-k-smoothing, the score is normalized with\n",
    "        a k value.\n",
    "        \"\"\"\n",
    "        context = self.check_context(context) # ngram의 길이보다 context가 항상 짧도록 검사\n",
    "        context_freqdist = self.ngrams[context] # context의 등장 횟수\n",
    "        word_count = context_freqdist[word] # context + word의 등장 횟수 : conditional frequency\n",
    "        context_count = context_freqdist.N() \n",
    "        return (word_count + self.k) / (context_count + self.k_norm)\n",
    "\n",
    "\n",
    "class LaplaceNgramModel(AddKNgramModel):\n",
    "    \"\"\"\n",
    "    Implements Laplace (add one) smoothing.\n",
    "    Laplace smoothing is the base case of Add-k smoothing,\n",
    "    with k set to 1.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args):\n",
    "        super(LaplaceNgramModel, self).__init__(1, *args)\n",
    "        \n",
    "        \n",
    "# BaseNgramModel -> AddK -> Laplace : AddK에 override 된 score 사용\n",
    "# BaseNgramModel -> KneserNeyModel : KneserNey에 override 된 score 사용\n",
    "\n",
    "\n",
    "\n",
    "class KneserNeyModel(BaseNgramModel):\n",
    "    \"\"\"\n",
    "    Implements Kneser-Ney smoothing\n",
    "    \"\"\"\n",
    "    def __init__(self, *args):\n",
    "        super(KneserNeyModel, self).__init__(*args) # counter 받아서 unigram/ngram/allgrams 생성\n",
    "        self.model = nltk.KneserNeyProbDist(self.ngrams)\n",
    "\n",
    "    def score(self, word, context):\n",
    "        \"\"\"\n",
    "        Use KneserNeyProbDist from NLTK to get score\n",
    "        \"\"\"\n",
    "        trigram = tuple((context[0], context[1], word))\n",
    "        return self.model.prob(trigram)\n",
    "\n",
    "    def samples(self):\n",
    "        return self.model.samples()\n",
    "\n",
    "    def prob(self, sample):\n",
    "        return self.model.prob(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The President of the United States\n",
      "This election year will mark\n"
     ]
    }
   ],
   "source": [
    "corpus = PickledCorpusReader('corpus_small')\n",
    "tokens = [''.join(word) for word in corpus.words()] \n",
    "vocab = Counter(tokens) # Counter: python 기본 제공 함수\n",
    "sents = list([word[0] for word in sent] for sent in corpus.sents()) # tag 제거, word[0]이 실제 단어!\n",
    "\n",
    "counter = count_ngrams(3, vocab, sents) #BaseNgramCounter instance\n",
    "knm = KneserNeyModel(counter)\n",
    "\n",
    "\n",
    "def complete(input_text):\n",
    "    tokenized = nltk.word_tokenize(input_text) # [The, President, of, the, United]\n",
    "    if len(tokenized) < 2:\n",
    "        response = \"Say more.\"\n",
    "    else:\n",
    "        completions = {}\n",
    "        \n",
    "        # 가능한 trigram sample 중에서 단어가 겹치는 것에 대해서 완성할 수 있는 단어 candidate : probability 딕셔너리 만들기\n",
    "        for sample in knm.samples(): # knm.samples() : trained data의 prob > 0 인 모든 trigram\n",
    "            if (sample[0], sample[1]) == (tokenized[-2], tokenized[-1]): # sample trigram 앞 두 단어 == input text의 마지막 두 단어\n",
    "                completions[sample[2]] = knm.prob(sample) # sample trigram의 probability\n",
    "        if len(completions) == 0:\n",
    "            response = \"Can we talk about something else?\"\n",
    "        else:\n",
    "            # dictionary 에서 가장 확률이 높은 candidate 찾기\n",
    "            best = max(\n",
    "                completions.keys(), key=(lambda key: completions[key]) # completions.keys() : candidates list\n",
    "            )\n",
    "            tokenized += [best] # tokenized list에 best 단어를 추가\n",
    "            response = \" \".join(tokenized)\n",
    "\n",
    "    return response\n",
    "\n",
    "print(complete(\"The President of the United\"))\n",
    "print(complete(\"This election year will\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ngram Language Models \n",
    "## Backgrounds\n",
    "-----\n",
    "### Significant Collocations\n",
    "#### nltk에서 significant collocations를 찾기 위해 사용하는 도구들\n",
    "* `CollocationFinder` :  collect collocation candidate frequencies, filter and rank them\n",
    "* `NgramAssocMeasuers`: generic association measures. Each public method returns a score.(Available methods : chi squre, jaccard, liklihood_ratio, mi_like, pmi, poisson_stirling, raw_freq, student_t...)\n",
    "\n",
    "#### `rank_quadgrams`\n",
    "* `ngrams = QuadgramCollocationFinder.from_words(corpus.words())`\n",
    "\t* `corpus.words()` -> corpus 내부의 Text가 word tokenization이 끝난 형태로 return\n",
    "    * `from_words`: `QuadgramCollocationFinder` class 내부의 함수. Construct a QuadgramCollocationFinder for n-grams(n<4) in the given sequence. \n",
    "    * `scored = ngrams.score_ngrams(metric)` : `metric`은 `QuadgramAssocMeasures`에서 제공하는 방법 중 하나. `metric`을 기준으로 더 중요한 ngram이 무엇인지 저장되어 `scored`에 저장된다. \n",
    "    \n",
    "``` \n",
    "# Full code\n",
    "def rank_quadgrams(corpus, metric, path=None):\n",
    "    \"\"\"\n",
    "    Find and rank quadgrams from the supplied corpus using the given\n",
    "    association metric. Write the quadgrams out to the given path if\n",
    "    supplied otherwise return the list in memory.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a collocation ranking utility from corpus words.\n",
    "    ngrams = QuadgramCollocationFinder.from_words(corpus.words())\n",
    "\n",
    "    # Rank collocations by an association metric\n",
    "    scored = ngrams.score_ngrams(metric)\n",
    "\n",
    "    if path:\n",
    "        with open(path, 'w') as f:\n",
    "            f.write(\"Collocation\\tScore ({})\\n\".format(metric.__name__))\n",
    "            for ngram, score in scored:\n",
    "                f.write(\"{}\\t{}\\n\".format(repr(ngram), score))\n",
    "    else:\n",
    "        return scored\n",
    "```\n",
    "따라서 \n",
    "```\n",
    "rank_quadgrams(\n",
    "        corpus, QuadgramAssocMeasures.likelihood_ratio, \"quadgrams.txt\"\n",
    "    )\n",
    "```\n",
    "와 같이 함수를 실행시키면 corpus의 quadgram들이 liklihood_ratio를 기준으로 정렬된 후 \"quadgrams.txt\" 파일에 순위 순으로 저장된다. \n",
    "\n",
    "\n",
    "#### `SignificantCollocations`\n",
    "* `BaseEstimator, TransformerMixin` : `sklearn` 라이브러리의 기본 클래스. preprocessing 과정에서 pipeline을 커스텀하기 위하여 사용한다. (참고: [BaseEstimator in sklearn.base](https://stackoverflow.com/questions/15233632/baseestimator-in-sklearn-base-python), [SCIKIT LEARN 전처리를 위한 변환기 만들기](https://databuzz-team.github.io/2018/11/11/make_pipeline/)\n",
    "* `def fit(self, docs, target)` : 문서 형태의 `docs`를 input으로 받아 ngram들을 형성하고 `ngrams`에 저장한다. 그리고 `self._scored_`에 `self.metric`을 기준으로 정렬된 significant collocations(ngrams)를 `dict` 형태로 저장한다. \n",
    "* `def transformation(self, docs, target)` : raw_freq가 높은 상위 50개의 ngram에 대하여 ngram과 그 score(fit method 에서 구한 점수)를 dict 형태로 저장한다.\n",
    "\n",
    "   \n",
    "      \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
